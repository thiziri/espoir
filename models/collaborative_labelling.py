import sys
import json
import os
from keras.layers import Input, Dense, Embedding, Concatenate
from keras.layers.merge import concatenate
from keras.models import Model
from keras.utils import plot_model
from utils import *
from keras import backend as K
from keras.layers import Lambda
from os.path import join
from tqdm import tqdm


if __name__ == '__main__':
    config_file = sys.argv[1]
    configure = json.load(open(config_file))
    config = configure["main_configuration"]
    config_data = config["data_sets"]
    config_model = config["model"]
    config_model_param = config_model["parameters"]
    config_model_train = config_model["train"]
    config_model_test = config_model["test"]
    print("Data extraction\nConfiguration: ")
    print(json.dumps(config, indent=2), end='\n')

    print("Read embeddings ...")
    embed_tensor = convert_embed_2_numpy(read_embedding(config_data["embed"]), config_data["vocab_size"])

    print("Create a model...")

    query = Input(name="in_query", shape=(config_data['query_maxlen'], ), dtype='int32')  # ex: query vector of 10 words
    doc = Input(name="in_doc", shape=(config_data['doc_maxlen'], ), dtype='int32')

    embedding = Embedding(config_data['vocab_size'], config_data['embed_size'], weights=[embed_tensor],
                          trainable=config_model_train['train_embed'], name="embeddings")  # load and/or train embeddings
    q_embed = embedding(query)
    d_embed = embedding(doc)
    print("Embedded inputs: \nq_embed: {qe}\nd_embed: {de}".format(qe=q_embed, de=d_embed))
    sum_dim1 = Lambda(lambda xin: K.sum(xin, axis=1), output_shape=(config_data['embed_size'],), name="sum_vectors")
    q_vector = sum_dim1(q_embed)  # (1 x embed_size)
    d_vector = sum_dim1(d_embed)  # (1 x embed_size)
    print("Added vectors\nq_vector: {qv}\nd_vector: {dv}".format(qv=q_vector, dv=d_vector))

    q_d_labels = Input(name="labels_vector", shape=(config_data['labelers_num'], ), dtype='float32')

    # 1- use one-hot encoding of the labels generated by each classical model and set number of judgements:
    # for judgements [0, 1, 2, 3], set number of judges for each flag: [0, 0, 2, 0] means that there are 2 classical
    # models that gave rel_label=2
    # use interval beans for labels=scores: [0, 0.5], [0.6, 0.7]...[0.9, 0.1]] create a vector where each dim is a
    # number of scores that are in this interval
    # https://stackoverflow.com/questions/51078625/how-to-ignore-some-input-layer-while-predicting-in-a-keras-model-trained-with

    input_vector = concatenate([q_vector, d_vector, q_d_labels])
    print("Concatenated vector: {iv}".format(iv=input_vector))
    dense = Dense(config_model_param["layers_size"][0], activation=config_model_param['hidden_activation'],
                  name="MLP_combine_0")(input_vector)
    for i in range(config_model_param["num_layers"]-2):
        dense = Dense(config_model_param["layers_size"][i], activation=config_model_param['hidden_activation'],
                      name="MLP_combine_"+str(i+1))(dense)
    dense = Dense(1, activation=config_model_param['output_activation'], name="MLP_out"+str(i+1))(dense)
    model = Model(inputs=[query, doc, q_d_labels], outputs=dense)
    model.compile(optimizer=config_model_param["optimizer"], loss=config_model_train["loss_function"],
                  metrics=config_model_train["metrics"])
    print(model.summary())
    plot_model(model, to_file=join(config_model_train["train_details"], 'collaborative.png'))
    # save model and resume

    print("Reading training data ...")
    x_train = []
    # input [arrays] each entry in x_train is a list of 3 lists [[]q,[]d, []l]:
    # list of q_word_ids, list of d_word_ids and list of automatically generated labels
    # details : https://keras.io/getting-started/functional-api-guide/#shared-layers

    print("Reading train instances ...")
    relation_labeler = {}
    relations = set()

    print("[First]:\nRead label files to relations...")
    for labeler_file in os.listdir(config_data["labels"]):
        relation_labeler[labeler_file] = {}
        with open(join(config_data["labels"], labeler_file), 'r') as labeler:
            for l in tqdm(labeler.readlines()):
                relation = (l.strip().split()[0], l.strip().split()[1])  # (q, d)
                relations.add(relation)
                relation_labeler[labeler_file][relation] = l.strip().split()[2]  # rel

    print("[Second]:\nSet relations as train instances...")
    for relation in tqdm(relations):
        q_words = []  # get q_word_ids from index
        d_words = []  # get d_word_ids from index
        rel_labels = []
        for labeler in relation_labeler:
            try:
                rel_labels.append(relation_labeler[labeler][relation])  # read the label given by labeler
            except:
                rel_labels.append(-1)  # set to -1 if the labeler does'nt gave any label

        x_train_i = [q_words, d_words, rel_labels]
        x_train.append(x_train_i)

    y_train = []  # output [array]



